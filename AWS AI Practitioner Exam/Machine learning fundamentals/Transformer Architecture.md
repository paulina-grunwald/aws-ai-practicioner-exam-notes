← [Back to Machine Learning Fundamentals](../Machine%20learning%20fundamentals.md)

# Transformer Architecture

Transformer Architecture

Transformer Architecture was developed by researchers at Google that is effective at Natural Language Processing (NLP) due to multi-head attention and positional encoding.

Transformer model architecture consists of two parts:


1. Encoder: reads and understands the input text.

It’s like a smart system that goes through everything it’s been taught (which is a lot of text) and picks up on the meanings of words and how they’re used in different contexts.


2. Decoder: Based on what the encoder has learned, this part generates new pieces of text.

It’s like a skilled writer that can make up sentences that flow well and make sense.

[Diagram: Decoder block with layers including Positional Encoding, Embeddings/Projections, Masked Multi-Headed Self-Attention, Multi-Headed Cross-Attention, Feed-Forward Network, and Normalization layers, leading to Predictions.]

