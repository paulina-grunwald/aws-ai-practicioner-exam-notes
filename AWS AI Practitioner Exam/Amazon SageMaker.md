← [Back to AWS AI Practitioner Exam](../AWS%20AI%20Practitioner%20Exam.md)

# Amazon Sagemaker

- **General**
    - **Amazon SageMaker** is a fully managed service provided by AWS that enables developers, data scientists, and machine learning (ML) engineers to build, train, and deploy machine learning models at scale. SageMaker simplifies the machine learning workflow by providing tools for every stage of the ML lifecycle, including data preparation, model training, deployment, and monitoring.
    - built to handle entire machine learning workflow
    
    ![image.png](Amazon%20Sagemaker/image.png)
    
    - End to End ML service
        - collect and prepare data
        - build and train machine learning models
        - deploy the models and monitor the performance of the predictions
- **Built-in Algorithms**
    - **Supervised Algorithms**
        - ***Linear regressions and classifications*** - A versatile algorithm for binary and multi-class classification and regression tasks.
        - **KNN Algorithms** (for classification)
    - **Unsupervised Algorithms**
        - **Principal Component Analysis (PCA)** – reduce the number of features
        - **K-means** – A clustering algorithm that groups data points into  k  clusters.
        - **Anomaly Detection**
    - Textual Algorithms – NLP, summarization…
    - Image Processing – classification, detection…
- **Automatic model tuning**
    - define the ***objective metric***
    - AMT automatically chooses hyperparameter ranges, search strategy, maximum runtime of a tuning job and early stop condion
    - saves you time and money
    - helps you not wasting money on suboptimal configurations
- **SageMaker JumpStart**
    - a feature within Amazon SageMaker that provides pre-built machine learning (ML) solutions, models, and tools to help users quickly get started with ML projects. It simplifies the process of building and deploying machine learning workflows by offering ready-to-use templates, pre-trained models, and example notebooks.
- **SageMaker Pipelines**
    - pipelines composed of steps and each step performs a specific task (e.g data preprocessing, model training)
    - Supported step types:
        - processing
        - training
        - tuning
        - autoML - automatically train a model
        - ClarifyCheck
        - QualityCheck
- **Deploying trained models**
    - save your model to S3
    - deploy in two ways
        - persistent endpoint for making individual predictions on demand
        - SageMaker Batch Transform to get forecasts for an entire dataset
    - Lots of cool options:
        - interference pipelines for more complex processing
        - SageMaker Neo for deploying of edge devices
        - Elastic Interference for accelerating deep learning models
        - automatic scaling (increase # of endpoints as needed)
        - Shadow Testing evaluates new models against currently deployed models to catch errors
- **Model deployment**
    - Deploy with one click, automatic scaling, no servers to manage (as opposed to self-hosted)
    - Managed solution: reduced overhead
    - real-time: one prediction at time
    - *Serverless*:
        - idle period between traffic spikes
        - can tolerate more latency (cold starts)
    - *Async*
        - for large payload sizes up to 1GB
        - long processing time
        - near-real time latency requirments
        - request and responses are in S3
    - *Batch*
        - prediction for an entire dataset (multiple predictions)
        - request and response are in S3
- **Shadow deployment**
    - *Shadow deployment* consists of releasing version B alongside version A, fork version A’s incoming requests, and send them to version B without impacting production traffic. This is particularly useful to test production load on a new feature and measure model performance on a new version without impacting current live traffic.
    
    https://aws.amazon.com/blogs/machine-learning/deploy-shadow-ml-models-in-amazon-sagemaker/
    
- **Scaling of Amazon Sage**
    
    Amazon SageMaker AI supports automatic scaling (auto scaling) for your hosted models. *Auto scaling* dynamically adjusts the number of instances provisioned for a model in response to changes in your workload. When the workload increases, auto scaling brings more instances online. When the workload decreases, auto scaling removes unnecessary instances so that you don't pay for provisioned instances that you aren't using.
    
- **Sage Maker Ground Truth**
    - designed to facilitate human-in-the-loop machine learning tasks, including collection and managin human feedback for techniques like Reinforcement Learning for Human Feedback
    - sometimes you don’t have training data at all, all it needs to be generated by human first
    - example: training an image classification model. somebody needs to tag a bunch of images with what they are  images before training a neutral network
    - Ground Truth manages humans who will label your data for training purposes
    - Ground Truth creates its own model as images are labeled by people
    - as this model learns, only images that model isn’t sure about are sent to human labelers
    - this can reduce cost of labeling jobs by 70%
    - Who are human labelers:
        - mechanical turk
        - your own internal team
        - professional labeling companies
    - Ground Truth Plus
    
    **References**
    
    https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/
    
- **Data Wrangler**
    - visual interface (in SageMaker Studio) to prepare data for machine learning
    - import data
    - visualize data
    - transform data (300+ transformations to choose from) or integrate your own custom forms with pandas, PySpark, PySpark SQL
    - “Quick Model” to train your model with your data and measure results
- **Feature Store**
    - a centralized repository to store, share and manage features for machine learning models
    - increased feature reusability: avoid redundant feature engineering efforts
    - improved model accuracy: access consistent and high-quality features
    - faster model development: streamline data preparation and feature engineering. enable feature sharing across teams.
    - fully managed service
    
    **Feature Group**: 
    
    - logical collection of related features that represent an entity or event (e.g customer features, product features)
    - key attributes
        - feature definitions: schema, defining, data types and descriptions
        - data sources: connections to data sources like S3, databases or streaming services
        - online/offline storage: configuration for online and offline store availbility
        - access control: manage permissions for features access and modifications
    - Offline and Online Stores: support for both real-time and batch inference
    - Online
    - Offline Store
    - Data ingestion: ingest data from various sources using batch of streaming methods
    - data validation and scheme enforcement
    - feature store is made of feature groups that contain
        - record identifier
        - feature name
        - event time
- **Model Monitor**
    - get alerts on quality deviations on your deployed models (via CloudWatch)
    - visualize data drift
        - e.g loan model starts giving people more credit due to drifting or missing input features
    - detect anomalies & outliers
    - detect new features
    - no code needed!
    - ***integrates with SageMaker Clarify***
    - data is stored in S3 and secured
    - monitoring jobs are scheduled via ***Monitoring Schedule***
    - Metrics are emitted to CloudWatch
        - CloudWatch notifications can be used to trigger alarms
        - You’d then take corrective action (retrain the model, audit the data)
    - Integrates with Tensorboard, QuickSight, Tableau
        - or visualize within SageMaker Studio
        
    - **Monitoring Types:**
        - Drift in data quality
        - Drift in model quality
        - Bias Drift
        - Feature attribution drift
- **SageMaker Clarify**
    - *detects potenial bias e.g imbalances* across different groups, ages, income brackets
    - With ModelMonitor, you can monitor for bias and be alerted to new potential bias via CloudWatch
    - SageMaker Clarify also helps explain model behaviour
        - understand which features contribute the most to your predictions
    - evaluates foundation models
    - evaluating human-factors such as friendliness or humor
    - leverage an AWS-managed team or bring your own employees
    - Use built-in datasets or bring your own dataset
    - Part of **SageMaker Studio**
    - *Model explainability:*
        - a set of tools to help explain how ML model makes predictions
        - understand model characteristics as a w hole prior to deployment
        - debug predictions provided by the model after it is deployed
        - helps increase the trust and understanding of the model
    - *Detect Bias:*
        - ability to detect and explain biases in your dataset and models
        - measure bias using statistical metrics
        - specify input features and bias will be automatically detected
    - *Ground Truth:*
        - RLHF - reinforcement learning from human feedback
            - model review, customization, and evaluation
            - align the  model to human preferences
            - reinforcement learning where human feedback is included in the reward function
        - Human Feedback for ML
            - create or evaluate your models
            - data generation or annotation (create labels)
        - ReviewersL Amazon Mechanical Turk workers, your employees or third-party vendors
        - SageMaker Ground Truth: tasks of data labeling
- **Cost managment**
    - Types of SageMaker costs:
        - notebook instance
        - hosting
- **Governance**
    - SageMaker Model Cards
        - Essential model information e.g intended uses, risk ratings, and training details
    - Model Dashboard:
        - centralized respository
        - information and insights for all models
    - Role Manger
        - define roles and personas
    - Model Monitor
        - monitor the quality of your model in prod: continuous or on-schedule
        - alerts for deviation in the model quality: fix data &retrain model
    - Model Registry
        - centralized repository allows you to track, manage and version ML models
        - catalog models, manage model versions, associate metadata with a model
        - Manage the approval status of a model, automate model deployment, and share models
- Automated ML
- Partial Dependence Plots (PDPs)
- **SageMaker Canvas**
    - no-code machine learning for business analysts
    - upload csv data (csv only for now), selected a column to predict, built it, and make predictions
    - can also join datatasets
    - classification or regression
    - automatic data cleaning
        - missing values
        - outliers
        - duplicates
    - share models & datasets with SageMaker Studio
    - also has generative AI support via Bedrock or JumpStart foundation models
        - many are fine-tuneable with Canvas

- **Network isolation mode**
    - Run SageMaker Job containers without any outbound internet access
    - can’t even access S3
- **SageMaker DeepAR forecasting algorithm:**
    - used to forecast time series data
    - leverages recurrent neutral network (RNN)

Automated ML

---

- **References**
    
    https://docs.aws.amazon.com/sagemaker/latest/dg/use-auto-ml.html
    
    https://aws.amazon.com/blogs/machine-learning/running-principal-component-analysis-in-amazon-sagemaker/
    
    [docs.aws.amazon.com](https://docs.aws.amazon.com/sagemaker/)