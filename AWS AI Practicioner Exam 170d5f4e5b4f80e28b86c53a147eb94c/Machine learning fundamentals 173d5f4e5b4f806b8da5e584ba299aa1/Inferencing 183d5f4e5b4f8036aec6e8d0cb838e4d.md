# Inferencing

![image.png](Inferencing%20183d5f4e5b4f8036aec6e8d0cb838e4d/image.png)

**Model inference**Â is when the trained model is used to generate predictions.

After the model has been trained, it is time to begin the process of using the information that a model has learned to make predictions or decisions. This is called inferencing.

There are two main types of inferencing in machine learning: ***batch inferencing*** and ***real-time inferencing.***

***Batch inferencing*** is when the computer takes a large amount of data, such as images or text, and analyzes it all at once to provide a set of results. This type of inferencing is often used for tasks like data analysis, where the speed of the decision-making process is not as crucial as the accuracy of the results.

***Real-time inferencing*** is when the computer has to make decisions quickly, in response to new information as it comes in. This is important for applications where immediate decision-making is critical, such as in chatbots or self-driving cars. The computer has to process the incoming data and make a decision almost instantaneously, without taking the time to analyze a large dataset.

Both batch and real-time inferencing have their own unique advantages and use cases. Your use case will determine which inferencing type you use.